@article{bassett2008hierarchical,
  title={Hierarchical organization of human cortical networks in health and schizophrenia},
  author={Bassett, Danielle S and Bullmore, Edward and Verchinski, Beth A and Mattay, Venkata S and Weinberger, Daniel R and Meyer-Lindenberg, Andreas},
  journal={Journal of Neuroscience},
  volume={28},
  number={37},
  pages={9239--9248},
  year={2008},
  publisher={Soc Neuroscience}
}


@article{girvan2002community,
  title={Community structure in social and biological networks},
  author={Girvan, Michelle and Newman, Mark EJ},
  journal={Proceedings of the national academy of sciences},
  volume={99},
  number={12},
  pages={7821--7826},
  year={2002},
  publisher={National Acad Sciences}
}


@article{carreno2017identifying,
  title={Identifying complex core--periphery structures in the interbank market},
  author={Carre{\~n}o, Jos{\'e} Gabriel and Cifuentes, Rodrigo},
  journal={Journal of Network Theory in Finance},
  year={2017}
}

@article{Gretton2012,
   abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distributionfree tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests. © 2012 Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf and Alexander Smola.},
   author = {Arthur Gretton and Karsten M. Borgwardt and Malte J. Rasch and Bernhard Schölkopf and Alexander Smola},
   issn = {15324435},
   journal = {Journal of Machine Learning Research},
   keywords = {Hypothesis testing,Integral probability metric,Kernel methods,Schema matching,Two-sample test,Uniform convergence bounds},
   pages = {723-773},
   title = {A kernel two-sample test},
   volume = {13},
   year = {2012},
}


@article{Kriege2020,
   abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner’s guide to kernel-based graph classification.},
   author = {Nils M. Kriege and Fredrik D. Johansson and Christopher Morris},
   doi = {10.1007/s41109-019-0195-3},
   issn = {23648228},
   issue = {1},
   journal = {Applied Network Science},
   keywords = {Graph kernels,Machine learning,Supervised graph classification},
   title = {A survey on graph kernels},
   volume = {5},
   year = {2020},
}

@article{Nikolentzos2019,
   abstract = {Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. During the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. The goal of this survey is to provide a unifying view of the literature on graph kernels. In particular, we present a comprehensive overview of a wide range of graph kernels. Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.},
   author = {Giannis Nikolentzos and Giannis Siglidis and Michalis Vazirgiannis},
   doi = {10.1613/jair.1.13225},
   month = {4},
   title = {Graph Kernels: A Survey},
   url = {http://arxiv.org/abs/1904.12218 http://dx.doi.org/10.1613/jair.1.13225},
   year = {2019},
}


@article{JMLR:v21:18-370,
  author  = {Giannis Siglidis and Giannis Nikolentzos and Stratis Limnios and Christos Giatsidis and Konstantinos Skianis and Michalis Vazirgiannis},
  title   = {GraKeL: A Graph Kernel Library in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {54},
  pages   = {1-5}
}


@article{Kang2012,
   abstract = {Random walk graph kernel has been used as an important tool for various data mining tasks including classi fication and similarity computation. Despite its usefulness, however, it suffers from the expensive computational cost which is at least O(n3) or O(m2) for graphs with n nodes and m edges. In this paper, we propose Ark, a set of fast algorithms for random walk graph kernel computation. Ark is based on the observation that real graphs have much lower intrinsic ranks, compared with the orders of the graphs. Ark exploits the low rank structure to quickly compute random walk graph kernels in O(n 2) or O(m) time. Experimental results show that our method is up to 97,865× faster than the existing algorithms, while providing more than 91.3% of the accuracies. Copyright © 2012 by the Society for Industrial and Applied Mathematics.},
   author = {U. Kang and Hanghang Tong and Jimeng Sun},
   doi = {10.1137/1.9781611972825.71},
   isbn = {9781611972320},
   journal = {Proceedings of the 12th SIAM International Conference on Data Mining, SDM 2012},
   pages = {828-838},
   title = {Fast random walk graph kernel},
   year = {2012},
}

@article{Togninalli2019,
   abstract = {Most graph kernels are an instance of the class of},
   author = {Matteo Togninalli and Elisabetta Ghisu and Felipe Llinares-López and Bastian Rieck and Karsten Borgwardt},
   title = {Wasserstein Weisfeiler-Lehman Graph Kernels},
   url = {http://arxiv.org/abs/},
   year = {2019},
}

@inproceedings{DK,
   abstract = {In this paper, we present Deep Graph Kernels, a unified framework to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information between sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve significant improvements in classification accuracy over state-of-the-art graph kernels.},
   author = {Pinar Yanardag and S V N Vishwanathan},
   city = {New York, NY, USA},
   doi = {10.1145/2783258.2783417},
   isbn = {9781450336642},
   journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {bioinformatics,collaboration networks,deep learning,graph kernels,r-convolution kernels,social networks,string kernels,structured data},
   pages = {1365–1374},
   publisher = {Association for Computing Machinery},
   title = {Deep Graph Kernels},
   url = {https://doi.org/10.1145/2783258.2783417},
   year = {2015},
}

@article{Du2019,
   abstract = {While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.},
   author = {Simon S. Du and Kangcheng Hou and Barnabás Póczos and Ruslan Salakhutdinov and Ruosong Wang and Keyulu Xu},
   issn = {23318422},
   issue = {NeurIPS},
   journal = {arXiv},
   title = {Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
   year = {2019},
}

@inproceedings{MONK,
   abstract = {Mean embeddings provide an extremely flexible and powerful tool in machine learning and statistics to represent probability distributions and define a semi-metric (MMD, maximum mean discrepancy; also called N-distance or energy distance), with numerous successful applications. The representation is constructed as the expectation of the feature map defined by a kernel. As a mean, its classical empirical estimator, however, can be arbitrary severely affected even by a single outlier in case of unbounded features. To the best of our knowledge, unfortunately even the consistency of the existing few techniques trying to alleviate this serious sensitivity bottleneck is unknown. In this paper, we show how the recently emerged principle of median-of-means can be used to design estimators for kernel mean embedding and MMD with excessive resistance properties to outliers, and optimal sub-Gaussian deviation bounds under mild assumptions.},
   author = {Matthieu Lerasle and Zoltan Szabo and Timothée Mathieu and Guillaume Lecue},
   editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
   journal = {Proceedings of the 36th International Conference on Machine Learning},
   pages = {3782-3793},
   publisher = {PMLR},
   title = {\{MONK\} Outlier-Robust Mean Embedding Estimation by Median-of-Means},
   volume = {97},
   url = {http://proceedings.mlr.press/v97/lerasle19a.html},
   year = {2019},
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}


@article{liu2009nonparanormal,
  title={The nonparanormal: Semiparametric estimation of high dimensional undirected graphs.},
  author={Liu, Han and Lafferty, John and Wasserman, Larry},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={10},
  year={2009}
}


@article{Orzechowski2019,
   abstract = {Biclustering is a technique that looks for patterns hidden in some columns and some rows of the input data. Evolutionary search-based biclustering (EBIC) is probably the first biclustering method that combines high accuracy of detection of multiple patterns with support for big data. EBIC has been recently extended to a multi-GPU method and allows to analyze very large datasets. In this short paper, we discuss the scalability of EBIC as well as its suitability for RNA-seq and single cell RNA-seq (scRNA-seq) experiments.},
   author = {Patryk Orzechowski and Jason H. Moore},
   doi = {10.1145/3319619.3326762},
   isbn = {9781450367486},
   journal = {GECCO 2019 Companion - Proceedings of the 2019 Genetic and Evolutionary Computation Conference Companion},
   keywords = {Biclustering,Data mining,Evolutionary computation,Genetic programming,Unsupervised machine learning},
   pages = {31-32},
   title = {EBIC: A scalable biclustering method for large scale data analysis},
   year = {2019},
}


@article{hagberg2020networkx,
  title={NetworkX: Network Analysis with Python},
  author={Hagberg, Aric and Conway, Drew},
  journal={URL: https://networkx. github. io},
  year={2020}
}


@article{friedman2008sparse,
  title={Sparse inverse covariance estimation with the graphical lasso},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  journal={Biostatistics},
  volume={9},
  number={3},
  pages={432--441},
  year={2008},
  publisher={Oxford University Press}
}